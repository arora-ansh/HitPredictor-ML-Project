{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y(dataY):\n",
    "    y = []\n",
    "    for i in range (len(dataY)):\n",
    "        if(dataY[i] > 50):\n",
    "            y.append(0)\n",
    "        else:\n",
    "            y.append(1)\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Explicit</th>\n",
       "      <th>Artist_followers</th>\n",
       "      <th>Track_number_on_album</th>\n",
       "      <th>Acousticness</th>\n",
       "      <th>Danceability</th>\n",
       "      <th>Energy</th>\n",
       "      <th>Liveness</th>\n",
       "      <th>Loudness</th>\n",
       "      <th>Speechiness</th>\n",
       "      <th>Tempo</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Key</th>\n",
       "      <th>Valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.112882</td>\n",
       "      <td>1.012971</td>\n",
       "      <td>-0.703623</td>\n",
       "      <td>-0.253508</td>\n",
       "      <td>-0.056900</td>\n",
       "      <td>-0.257351</td>\n",
       "      <td>-0.321798</td>\n",
       "      <td>-0.287111</td>\n",
       "      <td>1.295366</td>\n",
       "      <td>2.132090</td>\n",
       "      <td>0.865448</td>\n",
       "      <td>0.478965</td>\n",
       "      <td>0.209096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.898356</td>\n",
       "      <td>0.172558</td>\n",
       "      <td>-0.703623</td>\n",
       "      <td>0.798171</td>\n",
       "      <td>0.424214</td>\n",
       "      <td>-0.688588</td>\n",
       "      <td>-0.492682</td>\n",
       "      <td>0.282796</td>\n",
       "      <td>-0.811102</td>\n",
       "      <td>-0.910119</td>\n",
       "      <td>0.865448</td>\n",
       "      <td>0.752338</td>\n",
       "      <td>0.774957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.898356</td>\n",
       "      <td>-0.658039</td>\n",
       "      <td>-0.703623</td>\n",
       "      <td>0.764382</td>\n",
       "      <td>0.223151</td>\n",
       "      <td>0.783781</td>\n",
       "      <td>0.019972</td>\n",
       "      <td>0.893382</td>\n",
       "      <td>-0.654093</td>\n",
       "      <td>-0.677720</td>\n",
       "      <td>-1.155199</td>\n",
       "      <td>1.025710</td>\n",
       "      <td>1.138724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.898356</td>\n",
       "      <td>-0.471741</td>\n",
       "      <td>1.732316</td>\n",
       "      <td>-0.619272</td>\n",
       "      <td>-1.528965</td>\n",
       "      <td>0.506556</td>\n",
       "      <td>-0.410955</td>\n",
       "      <td>0.398452</td>\n",
       "      <td>-0.609112</td>\n",
       "      <td>2.593547</td>\n",
       "      <td>0.865448</td>\n",
       "      <td>0.752338</td>\n",
       "      <td>-1.555849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.898356</td>\n",
       "      <td>0.029319</td>\n",
       "      <td>-0.703623</td>\n",
       "      <td>-0.683893</td>\n",
       "      <td>0.330863</td>\n",
       "      <td>-1.138308</td>\n",
       "      <td>1.097289</td>\n",
       "      <td>-0.823916</td>\n",
       "      <td>-0.601473</td>\n",
       "      <td>-0.142817</td>\n",
       "      <td>0.865448</td>\n",
       "      <td>-1.434643</td>\n",
       "      <td>-1.806445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Explicit  Artist_followers  Track_number_on_album  Acousticness  \\\n",
       "0  1.112882          1.012971              -0.703623     -0.253508   \n",
       "1 -0.898356          0.172558              -0.703623      0.798171   \n",
       "2 -0.898356         -0.658039              -0.703623      0.764382   \n",
       "3 -0.898356         -0.471741               1.732316     -0.619272   \n",
       "4 -0.898356          0.029319              -0.703623     -0.683893   \n",
       "\n",
       "   Danceability    Energy  Liveness  Loudness  Speechiness     Tempo  \\\n",
       "0     -0.056900 -0.257351 -0.321798 -0.287111     1.295366  2.132090   \n",
       "1      0.424214 -0.688588 -0.492682  0.282796    -0.811102 -0.910119   \n",
       "2      0.223151  0.783781  0.019972  0.893382    -0.654093 -0.677720   \n",
       "3     -1.528965  0.506556 -0.410955  0.398452    -0.609112  2.593547   \n",
       "4      0.330863 -1.138308  1.097289 -0.823916    -0.601473 -0.142817   \n",
       "\n",
       "       Mode       Key   Valence  \n",
       "0  0.865448  0.478965  0.209096  \n",
       "1  0.865448  0.752338  0.774957  \n",
       "2 -1.155199  1.025710  1.138724  \n",
       "3  0.865448  0.752338 -1.555849  \n",
       "4  0.865448 -1.434643 -1.806445  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX = pd.read_csv('./datasets/spotofy_music_normalized.csv')\n",
    "dataY = pd.read_csv('./datasets/spotofy_music_labels.csv')\n",
    "dataY_multi = pd.read_csv('./datasets/spotofy_music_labels.csv')\n",
    "Y = convert_y(np.array(dataY))\n",
    "dataX.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement ANNs on the above data and see the accuracy scores of its prediction of hit vs not hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_dim=13, activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid')) #Single Layer Sigmoid Binary Classifier == Logistic Regression\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-92b082f334d5>:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "  estimator = KerasClassifier(build_fn=create_baseline, epochs=25, batch_size=5, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline with Un-Standardized Data, KFold with K = 5: 68.66% (1.39%)\n",
      "Baseline with Standardized Data, KFold with K = 5: 68.94% (1.08%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=25, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "results = cross_val_score(estimator, dataX, Y, cv=kfold)\n",
    "print(\"Baseline with Un-Standardized Data, KFold with K = 5: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "results2 = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Baseline with Standardized Data, KFold with K = 5: %.2f%% (%.2f%%)\" % (results2.mean()*100, results2.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network(activation = \"relu\", neurons = 10):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid')) #Single Layer Sigmoid Binary Classifier == Logistic Regression\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tushar\\AppData\\Local\\Temp/ipykernel_10844/3167065899.py:3: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "  estimators.append(('mlp', KerasClassifier(build_fn=create_neural_network, epochs=40, batch_size=5, verbose=1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.6386 - accuracy: 0.6582\n",
      "Epoch 2/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.6212 - accuracy: 0.6674\n",
      "Epoch 3/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.6085 - accuracy: 0.6729\n",
      "Epoch 4/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.6000 - accuracy: 0.6868\n",
      "Epoch 5/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5950 - accuracy: 0.6871\n",
      "Epoch 6/40\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.5908 - accuracy: 0.6883\n",
      "Epoch 7/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5881 - accuracy: 0.6912\n",
      "Epoch 8/40\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.5857 - accuracy: 0.6959\n",
      "Epoch 9/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5837 - accuracy: 0.7003\n",
      "Epoch 10/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5815 - accuracy: 0.6974\n",
      "Epoch 11/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5804 - accuracy: 0.7000\n",
      "Epoch 12/40\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.5786 - accuracy: 0.6991\n",
      "Epoch 13/40\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.5777 - accuracy: 0.7012\n",
      "Epoch 14/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5764 - accuracy: 0.7015\n",
      "Epoch 15/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5746 - accuracy: 0.7039\n",
      "Epoch 16/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5738 - accuracy: 0.7012\n",
      "Epoch 17/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5723 - accuracy: 0.7053\n",
      "Epoch 18/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5709 - accuracy: 0.7094\n",
      "Epoch 19/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5689 - accuracy: 0.7077\n",
      "Epoch 20/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5676 - accuracy: 0.7089\n",
      "Epoch 21/40\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.5681 - accuracy: 0.7130\n",
      "Epoch 22/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5660 - accuracy: 0.7109\n",
      "Epoch 23/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5654 - accuracy: 0.7086\n",
      "Epoch 24/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5649 - accuracy: 0.7127\n",
      "Epoch 25/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5634 - accuracy: 0.7130\n",
      "Epoch 26/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5630 - accuracy: 0.7103\n",
      "Epoch 27/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5629 - accuracy: 0.7106\n",
      "Epoch 28/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5613 - accuracy: 0.7083\n",
      "Epoch 29/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5614 - accuracy: 0.7124\n",
      "Epoch 30/40\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.5604 - accuracy: 0.7124\n",
      "Epoch 31/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5597 - accuracy: 0.7156\n",
      "Epoch 32/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5591 - accuracy: 0.7162\n",
      "Epoch 33/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5583 - accuracy: 0.7183\n",
      "Epoch 34/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5577 - accuracy: 0.7153\n",
      "Epoch 35/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5570 - accuracy: 0.7165\n",
      "Epoch 36/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5557 - accuracy: 0.7183\n",
      "Epoch 37/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5555 - accuracy: 0.7145\n",
      "Epoch 38/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5547 - accuracy: 0.7180\n",
      "Epoch 39/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5536 - accuracy: 0.7118\n",
      "Epoch 40/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5541 - accuracy: 0.7218\n",
      "170/170 [==============================] - 0s 1ms/step - loss: 0.5931 - accuracy: 0.7118\n",
      "Epoch 1/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.6352 - accuracy: 0.6671\n",
      "Epoch 2/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.6095 - accuracy: 0.6827\n",
      "Epoch 3/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.6015 - accuracy: 0.6874\n",
      "Epoch 4/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5971 - accuracy: 0.6832\n",
      "Epoch 5/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5935 - accuracy: 0.6883\n",
      "Epoch 6/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5906 - accuracy: 0.6936\n",
      "Epoch 7/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5896 - accuracy: 0.6915\n",
      "Epoch 8/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5873 - accuracy: 0.6962\n",
      "Epoch 9/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5860 - accuracy: 0.6962\n",
      "Epoch 10/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5843 - accuracy: 0.6965\n",
      "Epoch 11/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5813 - accuracy: 0.6936\n",
      "Epoch 12/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5812 - accuracy: 0.6956\n",
      "Epoch 13/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5782 - accuracy: 0.6977\n",
      "Epoch 14/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5780 - accuracy: 0.6983\n",
      "Epoch 15/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5757 - accuracy: 0.7024\n",
      "Epoch 16/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5743 - accuracy: 0.7050\n",
      "Epoch 17/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5728 - accuracy: 0.7033\n",
      "Epoch 18/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5715 - accuracy: 0.7065\n",
      "Epoch 19/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5708 - accuracy: 0.7047\n",
      "Epoch 20/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5701 - accuracy: 0.7059\n",
      "Epoch 21/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5682 - accuracy: 0.7097\n",
      "Epoch 22/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5674 - accuracy: 0.7039\n",
      "Epoch 23/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5675 - accuracy: 0.7089\n",
      "Epoch 24/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5653 - accuracy: 0.7139\n",
      "Epoch 25/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5646 - accuracy: 0.7086\n",
      "Epoch 26/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5650 - accuracy: 0.7124\n",
      "Epoch 27/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5638 - accuracy: 0.7115\n",
      "Epoch 28/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5635 - accuracy: 0.7115\n",
      "Epoch 29/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5631 - accuracy: 0.7130\n",
      "Epoch 30/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5626 - accuracy: 0.7133\n",
      "Epoch 31/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5617 - accuracy: 0.7165\n",
      "Epoch 32/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5611 - accuracy: 0.7136\n",
      "Epoch 33/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5614 - accuracy: 0.7136\n",
      "Epoch 34/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5608 - accuracy: 0.7150\n",
      "Epoch 35/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5602 - accuracy: 0.7124\n",
      "Epoch 36/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5589 - accuracy: 0.7139\n",
      "Epoch 37/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5587 - accuracy: 0.7162\n",
      "Epoch 38/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5574 - accuracy: 0.7162\n",
      "Epoch 39/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5571 - accuracy: 0.7209\n",
      "Epoch 40/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5577 - accuracy: 0.7195\n",
      "170/170 [==============================] - 1s 1ms/step - loss: 0.6215 - accuracy: 0.6776\n",
      "Epoch 1/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.6214 - accuracy: 0.6663\n",
      "Epoch 2/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.6025 - accuracy: 0.6883\n",
      "Epoch 3/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5952 - accuracy: 0.6901\n",
      "Epoch 4/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5908 - accuracy: 0.6960\n",
      "Epoch 5/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5882 - accuracy: 0.6989\n",
      "Epoch 6/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5852 - accuracy: 0.6998\n",
      "Epoch 7/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5824 - accuracy: 0.6986\n",
      "Epoch 8/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5809 - accuracy: 0.7019\n",
      "Epoch 9/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5794 - accuracy: 0.7051\n",
      "Epoch 10/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5772 - accuracy: 0.7087\n",
      "Epoch 11/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5763 - accuracy: 0.7072\n",
      "Epoch 12/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5749 - accuracy: 0.7125\n",
      "Epoch 13/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5738 - accuracy: 0.7087\n",
      "Epoch 14/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5717 - accuracy: 0.7084\n",
      "Epoch 15/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5714 - accuracy: 0.7087\n",
      "Epoch 16/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5689 - accuracy: 0.7131\n",
      "Epoch 17/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5684 - accuracy: 0.7089\n",
      "Epoch 18/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5666 - accuracy: 0.7163\n",
      "Epoch 19/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5660 - accuracy: 0.7125\n",
      "Epoch 20/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5647 - accuracy: 0.7142\n",
      "Epoch 21/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5642 - accuracy: 0.7113\n",
      "Epoch 22/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5614 - accuracy: 0.7157\n",
      "Epoch 23/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5619 - accuracy: 0.7163\n",
      "Epoch 24/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5622 - accuracy: 0.7137\n",
      "Epoch 25/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5601 - accuracy: 0.7151\n",
      "Epoch 26/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5594 - accuracy: 0.7172\n",
      "Epoch 27/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5575 - accuracy: 0.7204\n",
      "Epoch 28/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5579 - accuracy: 0.7151\n",
      "Epoch 29/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5555 - accuracy: 0.7187\n",
      "Epoch 30/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5546 - accuracy: 0.7195\n",
      "Epoch 31/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5548 - accuracy: 0.7210\n",
      "Epoch 32/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5558 - accuracy: 0.7151\n",
      "Epoch 33/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5547 - accuracy: 0.7192\n",
      "Epoch 34/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5529 - accuracy: 0.7207\n",
      "Epoch 35/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5521 - accuracy: 0.7184\n",
      "Epoch 36/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5532 - accuracy: 0.7225\n",
      "Epoch 37/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5511 - accuracy: 0.7201\n",
      "Epoch 38/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5501 - accuracy: 0.7234\n",
      "Epoch 39/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5490 - accuracy: 0.7207\n",
      "Epoch 40/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5493 - accuracy: 0.7290\n",
      "170/170 [==============================] - 0s 1ms/step - loss: 0.6325 - accuracy: 0.6749\n",
      "Epoch 1/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.6452 - accuracy: 0.6557\n",
      "Epoch 2/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.6146 - accuracy: 0.6804\n",
      "Epoch 3/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5998 - accuracy: 0.6910\n",
      "Epoch 4/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5925 - accuracy: 0.6975\n",
      "Epoch 5/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5887 - accuracy: 0.7034\n",
      "Epoch 6/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5856 - accuracy: 0.7025\n",
      "Epoch 7/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5818 - accuracy: 0.7010\n",
      "Epoch 8/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5806 - accuracy: 0.7063\n",
      "Epoch 9/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5779 - accuracy: 0.7084\n",
      "Epoch 10/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5759 - accuracy: 0.7072\n",
      "Epoch 11/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5739 - accuracy: 0.7107\n",
      "Epoch 12/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5731 - accuracy: 0.7087\n",
      "Epoch 13/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5721 - accuracy: 0.7119\n",
      "Epoch 14/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5691 - accuracy: 0.7110\n",
      "Epoch 15/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5691 - accuracy: 0.7142\n",
      "Epoch 16/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5684 - accuracy: 0.7101\n",
      "Epoch 17/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5665 - accuracy: 0.7151\n",
      "Epoch 18/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5654 - accuracy: 0.7184\n",
      "Epoch 19/40\n",
      "680/680 [==============================] - 1s 2ms/step - loss: 0.5647 - accuracy: 0.7181\n",
      "Epoch 20/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5652 - accuracy: 0.7157\n",
      "Epoch 21/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5633 - accuracy: 0.7160\n",
      "Epoch 22/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5627 - accuracy: 0.7160\n",
      "Epoch 23/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5614 - accuracy: 0.7172\n",
      "Epoch 24/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5605 - accuracy: 0.7195\n",
      "Epoch 25/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5586 - accuracy: 0.7190\n",
      "Epoch 26/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5590 - accuracy: 0.7195\n",
      "Epoch 27/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5571 - accuracy: 0.7190\n",
      "Epoch 28/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5572 - accuracy: 0.7210\n",
      "Epoch 29/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5565 - accuracy: 0.7207\n",
      "Epoch 30/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5560 - accuracy: 0.7175\n",
      "Epoch 31/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5554 - accuracy: 0.7172\n",
      "Epoch 32/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5540 - accuracy: 0.7245\n",
      "Epoch 33/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5528 - accuracy: 0.7204\n",
      "Epoch 34/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5542 - accuracy: 0.7192\n",
      "Epoch 35/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5519 - accuracy: 0.7222\n",
      "Epoch 36/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5520 - accuracy: 0.7222\n",
      "Epoch 37/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5520 - accuracy: 0.7234\n",
      "Epoch 38/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5508 - accuracy: 0.7225\n",
      "Epoch 39/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5500 - accuracy: 0.7260\n",
      "Epoch 40/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5503 - accuracy: 0.7251\n",
      "170/170 [==============================] - 0s 1ms/step - loss: 0.6273 - accuracy: 0.6855\n",
      "Epoch 1/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.6357 - accuracy: 0.6548\n",
      "Epoch 2/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.6039 - accuracy: 0.6901\n",
      "Epoch 3/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5963 - accuracy: 0.6928\n",
      "Epoch 4/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5923 - accuracy: 0.6945\n",
      "Epoch 5/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5895 - accuracy: 0.6992\n",
      "Epoch 6/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5871 - accuracy: 0.6954\n",
      "Epoch 7/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5850 - accuracy: 0.6986\n",
      "Epoch 8/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5822 - accuracy: 0.7042\n",
      "Epoch 9/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5820 - accuracy: 0.7001\n",
      "Epoch 10/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5811 - accuracy: 0.7054\n",
      "Epoch 11/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5795 - accuracy: 0.7031\n",
      "Epoch 12/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5781 - accuracy: 0.7072\n",
      "Epoch 13/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5773 - accuracy: 0.7066\n",
      "Epoch 14/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5757 - accuracy: 0.7081\n",
      "Epoch 15/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5754 - accuracy: 0.7095\n",
      "Epoch 16/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5741 - accuracy: 0.7125\n",
      "Epoch 17/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5725 - accuracy: 0.7131\n",
      "Epoch 18/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5724 - accuracy: 0.7092\n",
      "Epoch 19/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5709 - accuracy: 0.7092\n",
      "Epoch 20/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5708 - accuracy: 0.7157\n",
      "Epoch 21/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5694 - accuracy: 0.7101\n",
      "Epoch 22/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5692 - accuracy: 0.7119\n",
      "Epoch 23/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5678 - accuracy: 0.7122\n",
      "Epoch 24/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5671 - accuracy: 0.7145\n",
      "Epoch 25/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5656 - accuracy: 0.7142\n",
      "Epoch 26/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5654 - accuracy: 0.7139\n",
      "Epoch 27/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5642 - accuracy: 0.7113\n",
      "Epoch 28/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5630 - accuracy: 0.7131\n",
      "Epoch 29/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5621 - accuracy: 0.7128\n",
      "Epoch 30/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5625 - accuracy: 0.7184\n",
      "Epoch 31/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5617 - accuracy: 0.7128\n",
      "Epoch 32/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5608 - accuracy: 0.7166\n",
      "Epoch 33/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5595 - accuracy: 0.7148\n",
      "Epoch 34/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5593 - accuracy: 0.7169\n",
      "Epoch 35/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5572 - accuracy: 0.7198\n",
      "Epoch 36/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5573 - accuracy: 0.7139\n",
      "Epoch 37/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5566 - accuracy: 0.7119\n",
      "Epoch 38/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5555 - accuracy: 0.7175\n",
      "Epoch 39/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5554 - accuracy: 0.7216\n",
      "Epoch 40/40\n",
      "680/680 [==============================] - 1s 1ms/step - loss: 0.5552 - accuracy: 0.7178\n",
      "170/170 [==============================] - 0s 1ms/step - loss: 0.6203 - accuracy: 0.6843\n",
      "NN Accuracy: 68.68% (1.31%)\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "# estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_neural_network, epochs=40, batch_size=5, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "results = cross_val_score(pipeline, dataX, Y, cv=kfold)\n",
    "print(\"NN Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ReLU activation function on running for 25 epochs, the K-Fold CV accuracy on K=3 comes out to be 73.5% and the accuracy on K=5 is 71.5%. The Neural Network model trained was a simple one with one hidden layer with 5 neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the tanh activation function on running for 40 epochs, the K-Fold CV accuracy on K=3 comes out to be 66.89%. The Neural Network model trained was a simple one with one hidden layer with 10 neurons.\n",
    "and the accuracy on K=5 is 67.93% --> hiddlen layer with 5 neurons\n",
    "\n",
    "> Accuracy: 66.89% (1.32%)\n",
    "\n",
    ">Accuracy: 67.93% (0.89%)\n",
    "\n",
    ">Accuracy: 69.20% (0.90%) --> leaky_relu, 5 neurons, k=5\n",
    "\n",
    ">NN Accuracy: 68.75% (0.31%) --> leaky_relu, 5 neurons, k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67647058, 0.68470585, 0.67491168, 0.6949352 , 0.65135455])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
